{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  global verbose\n",
    "  if verbose:\n",
    "    print(predictions.shape, labels.shape)\n",
    "    verbose = False\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, depth=vocabulary_size):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, depth], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution(depth=vocabulary_size):\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, depth])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297808 learning rate: 10.000000\n",
      "(640, 27) (640, 27)\n",
      "Minibatch perplexity: 27.05\n",
      "================================================================================\n",
      "xe  rgituitoapeqezbsyjppmw get xetiqaitwsyfdbosmy  uinoeghdohafkdhwxuylgddlbecft\n",
      "dwozxc nm evsrpoxm tnapeamd i oevmava br rbonuecg awt at xzln cyw tfzkiooie  efd\n",
      "vqjfnt kcs rlt jcwearvmdvr j hzsetnbtmldremnpv zrcetrfwhsunecsteva ttcqekxpd eix\n",
      "jeevnrhat ctdtg elseltu   ltlhi gbqao ag vtnhrmsjvlmjlatdh t sleku  mhxacigusxi \n",
      " ekpl c  juhjz m su gxf ew fkqfxim dircrusaieplkaflrtfgaowh  pktehrejrriie tiwie\n",
      "================================================================================\n",
      "Validation set perplexity: 20.29\n",
      "Average loss at step 100: 2.598798 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.96\n",
      "Validation set perplexity: 10.40\n",
      "Average loss at step 200: 2.250192 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.65\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 300: 2.099673 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 400: 2.003008 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 500: 1.940410 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 600: 1.910406 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 700: 1.861992 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 800: 1.821987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 900: 1.833734 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 1000: 1.825705 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "dy equecish he somolections were six tecender of free undantion recitiol le will\n",
      "ate in a brema with lince vore be edfere lary issianif perdermentent devisted he\n",
      "jums with to monn the pyoliom willed alterndy two five atooriay is efflication t\n",
      "haming enrile persidef refourd lesmering featia oreard vioged way the beas reger\n",
      "t one one encate he wecreopt undent in rich anownter in one nine eight and ceppe\n",
      "================================================================================\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1100: 1.777822 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1200: 1.750493 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1300: 1.729327 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1400: 1.745095 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1500: 1.731273 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1600: 1.739481 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1700: 1.711025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1800: 1.672271 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1900: 1.645911 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2000: 1.695704 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "zana partsion sisponts sof stacty with eallane iceirism mintmandmpicance cogelin\n",
      "k lininucix model de woss a betwuilist to knglish xoking that altheats was only \n",
      "quish peviar films in alker spicces are invent in one four sbandack and to bease\n",
      "bot is in immaracting ordemp laticalical of houst samment od denains reprodes da\n",
      "ed to headd atgliin yegiin jotan eride and these eastical two clbarcerog babriti\n",
      "================================================================================\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2100: 1.684940 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2200: 1.681107 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2300: 1.639650 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2400: 1.659049 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2500: 1.680044 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2600: 1.654860 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2700: 1.658554 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2800: 1.653252 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2900: 1.651646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3000: 1.651211 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "le in paruinct wordnk the been crossic any alliaged ab the socificuad two zero z\n",
      "phich use he quars an opaner one nine three victives umstive caspla saccle one n\n",
      "y influenceld and mangesman at resboce proace is to coth made accoun famal infou\n",
      "k inther coupentary the councims frowks same f arglagioss werkud by was bus and \n",
      "p son flypored blogal version wattored of one nine two z a thatistical eight two\n",
      "================================================================================\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3100: 1.627068 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3200: 1.646360 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3300: 1.639233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3400: 1.670512 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3500: 1.658920 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3600: 1.669765 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3700: 1.647910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3800: 1.646028 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3900: 1.636073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4000: 1.651774 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "uce the his or nightopmay al wad rymerstence el betwern intrydiate becompredeter\n",
      "jander spankted edfleceted there emplets obseal is ethres the caicufrests from w\n",
      "neoning spous to one zero rivide one eight eight one four three zero zero one ze\n",
      "gees recigences french one nine three five zero zero zero zero esposed iffective\n",
      "lation with their begins ane world meazomzail two no deaenian mgdolos the role t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4100: 1.633491 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4200: 1.632128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4300: 1.612845 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4400: 1.608224 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4500: 1.616896 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4600: 1.613160 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4700: 1.627137 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4800: 1.629791 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4900: 1.632886 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5000: 1.607129 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "ut force often accorded one wests cultuel wikito must vankes or be desserent nor\n",
      "wel oldism the is no to his langua is one hissolorijes of the entular few mates \n",
      "fipujinglius in or english however haprise very of are larg ow he quedion one ol\n",
      "s u in in mays auto of a lineyte grince the iiring winds they schithe with devom\n",
      "bopixia commont son deparahi instivite consitions assopist oviliation greach x p\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5100: 1.605801 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5200: 1.587564 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5300: 1.574902 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5400: 1.576841 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5500: 1.564283 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5600: 1.580183 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5700: 1.567539 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5800: 1.580752 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5900: 1.577202 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6000: 1.547189 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "quaration beschoutic ramions may in javwiding their there as chricts is discoven\n",
      "zen are one five seven and belike was the one nine four five th one six th a nam\n",
      "actests action aborcheverse co phositice welsh to showhoon misirdan and prisos s\n",
      "s of not io on firties free one with for at one nine seven four five eight four \n",
      "s of the commone cule court event ropida aftistion wimmoble influence four th li\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100: 1.565721 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6200: 1.535490 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6300: 1.549471 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6400: 1.540595 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6500: 1.559727 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6600: 1.601493 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6700: 1.578253 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6800: 1.601429 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6900: 1.581314 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 7000: 1.575312 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "s militared in testark curewarx air the decilling and two one emphorows faibal n\n",
      "nators mon hench over save to grand known blocal in an omilelly one nine two boo\n",
      "warestovery rush for life pay commonce discourbs presotrable war trypterk as nor\n",
      " lard in fiction when thirbutation slyincasgia in dices groapus is alsysion acqu\n",
      "vertion front cynines out consyarting socdess explans capid into surgutes poods \n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # The big matrix\n",
    "  Tw = tf.Variable(tf.truncated_normal([vocabulary_size + num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  Tb = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, state, cell):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    i = tf.concat([i, state], 1)\n",
    "    tmp = tf.matmul(i, Tw) + Tb\n",
    "    \n",
    "    inputs_gate = tf.sigmoid(tmp[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(tmp[:, num_nodes:2*num_nodes])\n",
    "    output_gate = tf.sigmoid(tmp[:, 2*num_nodes:3*num_nodes])\n",
    "    update_gate = tf.tanh(tmp[:, 3*num_nodes:])\n",
    "    \n",
    "    cell = forget_gate * cell + inputs_gate * update_gate\n",
    "    state = output_gate * tf.tanh(cell)\n",
    "    return state, cell\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298967 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.08\n",
      "================================================================================\n",
      "khepybinbtjigrtx eoi y    ghs rny g h mt use qk rt unvr  ar  tqbdsweoc nzo    ti\n",
      "qbitekf acgw qohxed otssmnrkl snq wxzzlbdqt igvekniv raasa b ifz   cre eeirbham \n",
      "wvqzsspcyxcgtarmyd n kgltjekgbtewlmja crvmpttiisrtrhs vearndythraip  txua s  uwb\n",
      "neieeizggbo qbkhtlqo e oeyrqvrtvzze fs swa snaadnsowq dnj b  rrcg  rkckejuvira v\n",
      "ddzw if r uml  l d qvbtno pr sefqedl us tadevokrnme nxejrfeyx kzbavae njcavifpd \n",
      "================================================================================\n",
      "Validation set perplexity: 20.07\n",
      "Average loss at step 100: 2.580508 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.52\n",
      "Validation set perplexity: 10.92\n",
      "Average loss at step 200: 2.242405 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.31\n",
      "Validation set perplexity: 8.64\n",
      "Average loss at step 300: 2.089513 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 400: 2.034798 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.94\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 500: 1.979261 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 600: 1.897382 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 700: 1.870845 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 800: 1.868608 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 900: 1.840806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 1000: 1.841673 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "================================================================================\n",
      "s workno from he a to thefe in in p lent of corprecins impoce and nom comses exa\n",
      "kacce and has raki he pipinarde obounss forsamem schrence one zero fith the mide\n",
      "k l g applory amen we hygian usermide by the abbored on gavinys the garof of eng\n",
      "wors not to setins stards olch signomman magitari wad labilia voses itmebkicate \n",
      "ffenc which dopuled ouccom vert reposed hinish nordd by ander wiplo or grougboun\n",
      "================================================================================\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1100: 1.793815 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1200: 1.765492 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1300: 1.753946 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1400: 1.752830 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1500: 1.739506 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1600: 1.722057 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1700: 1.710851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1800: 1.683718 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1900: 1.687882 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2000: 1.670376 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "ja requeitisul frep hit exented mock after sti elling all out of ever hen gwad t\n",
      "os kast almorank abserzer in the comman shest of nexsone is outch jativents with\n",
      "cepsuy four seven five juzo shepedaer to the dian kays and in anspegibrian ims f\n",
      "e cater that have armugs the usivers athas that tw two zero zero two zero brofof\n",
      "s stidons a the difeation is declany over action effically if greacly spension i\n",
      "================================================================================\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2100: 1.681739 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2200: 1.697204 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2300: 1.696948 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2400: 1.675800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2500: 1.677184 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2600: 1.659984 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2700: 1.674076 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2800: 1.673883 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2900: 1.667571 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 3000: 1.676146 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "ne of the zero but art or s of author cbincher and op somel recusrus thats then \n",
      "alitist be the steppess of twi highched conceptual convities feld threate engea \n",
      "jer is terped us the productored it as chests hagn armment contress west peecrif\n",
      "riplosor negal we stornets celers generation other was iars many woak tempat by \n",
      "quired it over to releppent is purt the one six a served to tere of presidest ex\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3100: 1.644696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3200: 1.623944 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3300: 1.635622 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3400: 1.624943 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3500: 1.662833 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3600: 1.641492 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3700: 1.641135 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3800: 1.647832 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3900: 1.644290 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4000: 1.625498 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "s claue the tonadia lever from politaias the from examples to founders two nine \n",
      "ist the europeane of pamenger and coupt zecome dutibal chins heid the five the t\n",
      "x toma two one four any the real eghthen assendars of economivian incapinagess m\n",
      "chety has the mispper of types reflects is provires kaman darachald is dead on m\n",
      "back been and fug an eight the europeays detistors to oft dirating the mamubaban\n",
      "================================================================================\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4100: 1.608839 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4200: 1.604134 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4300: 1.610403 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4400: 1.599063 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4500: 1.625331 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4600: 1.610387 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4700: 1.613389 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4800: 1.592271 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4900: 1.607937 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5000: 1.604098 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "ment telectensite pets hid on the armes one nine birensoul dance once were exper\n",
      "nts s pmigroungs two nine one nine five one xign traved to the fandom gods greaj\n",
      "zlor rander for zerd to conurets the news gomember polless callion it is new wer\n",
      "vernombers reject of joal hade manyy to howeveresed terest lopbomianed a feature\n",
      "hisds breed of the for all works some for the countryss were all presented strap\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5100: 1.577972 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5200: 1.578464 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5300: 1.580379 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5400: 1.575866 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5500: 1.574467 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5600: 1.548535 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5700: 1.563779 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5800: 1.587782 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5900: 1.566688 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6000: 1.572205 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "dism two itsecon groups unstations what is the siz the war actoromine all or a w\n",
      "fores d techaming codity berular shipla sabilifically appeer ronking micnabionze\n",
      "quance name cultion viscilia egimind lewt an attentus vernical iir complens aspu\n",
      "was setarion distribe line judicary was is ileminik to the truns and cude male h\n",
      "time in tone only follown the number who mets story s on populilia and dhriganne\n",
      "================================================================================\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6100: 1.561915 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6200: 1.574355 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6300: 1.570911 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6400: 1.561005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6500: 1.542737 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6600: 1.585608 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6700: 1.555855 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6800: 1.561213 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6900: 1.555613 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 7000: 1.571331 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "ish partician chinesh the michanorian the numbers astical istanse it wifter firs\n",
      "x foxcet of the joi of my villed pole repinid very reference truduity and german\n",
      "ys are yds into the compain woaturamor bandloyodrae dee named prynas to haid out\n",
      "vel abolispated to the hay have mommer appre ponery eucht and high the sonts x c\n",
      "x exp two zero two englack whis substery of the strugio and bolk development eve\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bigrams: 728 of 756 possibly existing\n"
     ]
    }
   ],
   "source": [
    "bigrams = list(set([''.join(t) for t in zip(text, text[1:])]))\n",
    "n_bigrams = len(bigrams)\n",
    "print('Number of bigrams: %d of 756 possibly existing' % n_bigrams)\n",
    "\n",
    "bi2id = {}\n",
    "id2bi = {}\n",
    "\n",
    "for i, b in enumerate(bigrams):\n",
    "    id2bi[i] = b\n",
    "    bi2id[b] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // (2 * batch_size)\n",
    "    self._cursor = [ offset * segment for offset in range(2 * batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = bi2id[self._text[self._cursor[b]] + self._text[self._cursor[b] + 1]]\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bi_train_batches = BiBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "bi_valid_batches = BiBatchGenerator(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ons anarchists advocat'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = bi_train_batches.next()\n",
    "tmp = ''\n",
    "for i in range(len(b)):\n",
    "    tmp += id2bi[b[i][0]]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iece of his'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = train_batches.next()\n",
    "tmp = ''\n",
    "for i in range(len(b)):\n",
    "    tmp += id2char(np.argmax(b[i][0]))\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(values, depth):\n",
    "    assert (values < depth).all(), 'Incorrect value for given depth'\n",
    "    res = np.zeros([len(values), depth], dtype=np.float32)\n",
    "    for i, v in enumerate(values):\n",
    "        res[i][v] = 1.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Embedding\n",
    "  embeddings = tf.Variable(tf.truncated_normal([n_bigrams, embedding_size], -0.1, 0.1), name='embeddings')\n",
    "  # The big matrix\n",
    "  Tw = tf.Variable(tf.truncated_normal([embedding_size + num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  Tb = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, n_bigrams], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([n_bigrams]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, state, cell):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    i = tf.concat([i, state], 1)\n",
    "    tmp = tf.matmul(i, Tw) + Tb\n",
    "    \n",
    "    inputs_gate = tf.sigmoid(tmp[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(tmp[:, num_nodes:2*num_nodes])\n",
    "    output_gate = tf.sigmoid(tmp[:, 2*num_nodes:3*num_nodes])\n",
    "    update_gate = tf.tanh(tmp[:, 3*num_nodes:])\n",
    "    \n",
    "    cell = forget_gate * cell + inputs_gate * update_gate\n",
    "    state = output_gate * tf.tanh(cell)\n",
    "    return state, cell\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    e = tf.reshape(tf.nn.embedding_lookup(embeddings, i), [batch_size, -1])\n",
    "    sparse_e = e # tf.nn.dropout(e, keep_prob=0.5)\n",
    "    output, state = lstm_cell(sparse_e, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    sparse_output = tf.nn.dropout(tf.concat(outputs, 0), keep_prob=0.7)\n",
    "    logits = tf.nn.xw_plus_b(sparse_output, w, b)\n",
    "    true = tf.one_hot(indices=tf.concat(train_labels, 0), depth=n_bigrams)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=true, logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 3000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_embedding = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input), [1, -1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.624696 learning rate: 10.000000\n",
      "Minibatch perplexity: 753.48\n",
      "================================================================================\n",
      "--> pncieukxxvvbftrviyhkrddwlqbvwklfzyvomdgaqjjhhjearjrhkdrbruouenspampssomivqikbwahqq\n",
      "--> efxqrslhrlmjjmqjzqp qsjknirelomh nlncijhopedvevec  avq ckkwubrqgpciakwnumilnyomreq\n",
      "--> lonqvsmv he uhtlrpoptcshwm kzqcfvdt tukgpyqozavjuisuqmquawztpjvuxyqoble iewaqqgs k\n",
      "--> jaszkhdmcnriiyhwuemocphewwciovnsbfveavocbrnbxcgdlti arbynkfgmveinjwfsykbdahjwce mf\n",
      "--> fzvlqemczvjdmcdlpoccsklnmn zod borejfmar vfwrsrsutgdouktoyxvqgrybbxmalrjrrdbgtocby\n",
      "================================================================================\n",
      "Validation set perplexity: 535.61\n",
      "Average loss at step 100: 5.331909 learning rate: 10.000000\n",
      "Minibatch perplexity: 149.29\n",
      "Validation set perplexity: 137.31\n",
      "Average loss at step 200: 4.667777 learning rate: 10.000000\n",
      "Minibatch perplexity: 77.85\n",
      "Validation set perplexity: 91.77\n",
      "Average loss at step 300: 4.263877 learning rate: 10.000000\n",
      "Minibatch perplexity: 64.97\n",
      "Validation set perplexity: 67.85\n",
      "Average loss at step 400: 4.055407 learning rate: 10.000000\n",
      "Minibatch perplexity: 53.42\n",
      "Validation set perplexity: 54.33\n",
      "Average loss at step 500: 3.932853 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.46\n",
      "Validation set perplexity: 48.18\n",
      "Average loss at step 600: 3.861344 learning rate: 10.000000\n",
      "Minibatch perplexity: 49.10\n",
      "Validation set perplexity: 44.27\n",
      "Average loss at step 700: 3.826910 learning rate: 10.000000\n",
      "Minibatch perplexity: 49.14\n",
      "Validation set perplexity: 43.21\n",
      "Average loss at step 800: 3.785156 learning rate: 10.000000\n",
      "Minibatch perplexity: 47.52\n",
      "Validation set perplexity: 40.15\n",
      "Average loss at step 900: 3.719016 learning rate: 10.000000\n",
      "Minibatch perplexity: 43.55\n",
      "Validation set perplexity: 37.47\n",
      "Average loss at step 1000: 3.664612 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.53\n",
      "================================================================================\n",
      "--> bxndly obsentains is a geosbery rul sen in nine thail can mundle is anarwl sgzoins\n",
      "--> yx veu the wering apeined ating of the axetials of the cone epiters hin the collo \n",
      "--> py hpd san comporto the v actions as the one fhyal and carfooid divictions the tic\n",
      "--> qobil free tfunt speeoetanm s including the fullian acgolijvia oneart is chancutic\n",
      "--> ry othernmenmas evea mays mughy the overmibcs in the acial ares anlatial for the h\n",
      "================================================================================\n",
      "Validation set perplexity: 36.79\n",
      "Average loss at step 1100: 3.654370 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.07\n",
      "Validation set perplexity: 36.51\n",
      "Average loss at step 1200: 3.627567 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.56\n",
      "Validation set perplexity: 32.66\n",
      "Average loss at step 1300: 3.576149 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.33\n",
      "Validation set perplexity: 30.45\n",
      "Average loss at step 1400: 3.541084 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.08\n",
      "Validation set perplexity: 29.04\n",
      "Average loss at step 1500: 3.579647 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.35\n",
      "Validation set perplexity: 29.70\n",
      "Average loss at step 1600: 3.562094 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.26\n",
      "Validation set perplexity: 29.69\n",
      "Average loss at step 1700: 3.518040 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.00\n",
      "Validation set perplexity: 27.10\n",
      "Average loss at step 1800: 3.498905 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.50\n",
      "Validation set perplexity: 26.93\n",
      "Average loss at step 1900: 3.522613 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.42\n",
      "Validation set perplexity: 26.88\n",
      "Average loss at step 2000: 3.496514 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.84\n",
      "================================================================================\n",
      "--> zq also has meriyweened and at and places has so aftcary about to which speasing s\n",
      "--> yut the crearfay holt tey the grive spottorn and as two veuter che took citing as \n",
      "--> ain s bas s sciric methe and conlar generan druir boxf same as jomout interfharati\n",
      "--> than beh furet it sat to a s lost commated to inu roman xkstan of the my ed and an\n",
      "--> iars in escens to prothat uneclation and or the emmassater bolating s ortnies taul\n",
      "================================================================================\n",
      "Validation set perplexity: 26.52\n",
      "Average loss at step 2100: 3.483848 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.53\n",
      "Validation set perplexity: 26.10\n",
      "Average loss at step 2200: 3.433290 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.16\n",
      "Validation set perplexity: 26.51\n",
      "Average loss at step 2300: 3.431975 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.52\n",
      "Validation set perplexity: 24.29\n",
      "Average loss at step 2400: 3.487099 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.33\n",
      "Validation set perplexity: 25.21\n",
      "Average loss at step 2500: 3.428766 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.44\n",
      "Validation set perplexity: 25.41\n",
      "Average loss at step 2600: 3.390788 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.04\n",
      "Validation set perplexity: 24.42\n",
      "Average loss at step 2700: 3.469889 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.39\n",
      "Validation set perplexity: 23.39\n",
      "Average loss at step 2800: 3.472542 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.13\n",
      "Validation set perplexity: 23.18\n",
      "Average loss at step 2900: 3.398306 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.75\n",
      "Validation set perplexity: 23.13\n",
      "Average loss at step 3000: 3.434526 learning rate: 1.000000\n",
      "Minibatch perplexity: 36.53\n",
      "================================================================================\n",
      "--> nnus one in butievers diplies indistive zero soudies contermed concludaia the conc\n",
      "--> qtlo substed centory for lamned the american devedice major buities dejectoral per\n",
      "--> wzg theoneetes spected as fat but occangy sevary buarkting scill dud s no it mable\n",
      "--> tus of msembraoity togic alosafore with east appla parlties gumifond bn tennbith n\n",
      "--> yl influent results comporing the  spreash ufred in alltip church confricerated in\n",
      "================================================================================\n",
      "Validation set perplexity: 23.50\n",
      "Average loss at step 3100: 3.422059 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.87\n",
      "Validation set perplexity: 22.89\n",
      "Average loss at step 3200: 3.413426 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.91\n",
      "Validation set perplexity: 22.19\n",
      "Average loss at step 3300: 3.400071 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.53\n",
      "Validation set perplexity: 21.78\n",
      "Average loss at step 3400: 3.396578 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.59\n",
      "Validation set perplexity: 21.89\n",
      "Average loss at step 3500: 3.404624 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.09\n",
      "Validation set perplexity: 21.49\n",
      "Average loss at step 3600: 3.376063 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.38\n",
      "Validation set perplexity: 21.51\n",
      "Average loss at step 3700: 3.462551 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.83\n",
      "Validation set perplexity: 21.54\n",
      "Average loss at step 3800: 3.409700 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.32\n",
      "Validation set perplexity: 21.48\n",
      "Average loss at step 3900: 3.391013 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.12\n",
      "Validation set perplexity: 21.40\n",
      "Average loss at step 4000: 3.378838 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.35\n",
      "================================================================================\n",
      "--> wlin often selnia the mecorbated on they lises lows in the frent to imploped iqxas\n",
      "-->  america rational that it find such enter in essiallation g what universionary of \n",
      "--> ae suppossentral as food internal mountary accoralis influes although auth with in\n",
      "--> xcesrration anakteurr by the sinvigunaged attempra actearicable bake hect nometor \n",
      "--> ium but cotct dukic many use to exeveration of the highly system locest to imoole \n",
      "================================================================================\n",
      "Validation set perplexity: 21.32\n",
      "Average loss at step 4100: 3.407946 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.62\n",
      "Validation set perplexity: 21.43\n",
      "Average loss at step 4200: 3.474715 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.41\n",
      "Validation set perplexity: 21.49\n",
      "Average loss at step 4300: 3.411042 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.71\n",
      "Validation set perplexity: 21.39\n",
      "Average loss at step 4400: 3.419081 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.95\n",
      "Validation set perplexity: 21.56\n",
      "Average loss at step 4500: 3.409616 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.32\n",
      "Validation set perplexity: 21.42\n",
      "Average loss at step 4600: 3.372508 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.68\n",
      "Validation set perplexity: 21.18\n",
      "Average loss at step 4700: 3.378292 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.92\n",
      "Validation set perplexity: 21.20\n",
      "Average loss at step 4800: 3.367986 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.15\n",
      "Validation set perplexity: 21.28\n",
      "Average loss at step 4900: 3.313120 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.62\n",
      "Validation set perplexity: 21.17\n",
      "Average loss at step 5000: 3.305752 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.53\n",
      "================================================================================\n",
      "--> lpory multell the reactu tip seconet waphy three conderless number the the coli an\n",
      "--> vb roredt five one nine six eight ur communication four porredian unshipidkening t\n",
      "--> bbies five he publical rear university the herject serve vieture a hould an it aci\n",
      "--> mn wis and re supprown one six for refroned by oneer of the new and diience of the\n",
      "--> nbths do is most to whore he for to times two cesses ancescular noponoquentation i\n",
      "================================================================================\n",
      "Validation set perplexity: 21.19\n",
      "Average loss at step 5100: 3.343741 learning rate: 1.000000\n",
      "Minibatch perplexity: 35.81\n",
      "Validation set perplexity: 21.20\n",
      "Average loss at step 5200: 3.361613 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.88\n",
      "Validation set perplexity: 21.22\n",
      "Average loss at step 5300: 3.392845 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.78\n",
      "Validation set perplexity: 20.96\n",
      "Average loss at step 5400: 3.374189 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.98\n",
      "Validation set perplexity: 20.99\n",
      "Average loss at step 5500: 3.354741 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.39\n",
      "Validation set perplexity: 20.97\n",
      "Average loss at step 5600: 3.332642 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.22\n",
      "Validation set perplexity: 20.81\n",
      "Average loss at step 5700: 3.353159 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.90\n",
      "Validation set perplexity: 21.08\n",
      "Average loss at step 5800: 3.340642 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.20\n",
      "Validation set perplexity: 21.14\n",
      "Average loss at step 5900: 3.294687 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.59\n",
      "Validation set perplexity: 21.08\n",
      "Average loss at step 6000: 3.344031 learning rate: 0.100000\n",
      "Minibatch perplexity: 27.49\n",
      "================================================================================\n",
      "--> ih hackering in can ratuing sabimatics of anrofirsters the albernollal wooks class\n",
      "--> nrue brancal fewauloch ill labosenitics meased production of relean the emitived f\n",
      "--> xvh and qualist dirodolmages lout moel means in one four two barching the largish \n",
      "--> dt in exsfirces and mavie oosteved to final norma of percent viking out of crutara\n",
      "--> gpd are rats peis adde of vaarhmler governed to an fp her more truissism strat and\n",
      "================================================================================\n",
      "Validation set perplexity: 21.22\n",
      "Average loss at step 6100: 3.386558 learning rate: 0.100000\n",
      "Minibatch perplexity: 31.87\n",
      "Validation set perplexity: 21.17\n",
      "Average loss at step 6200: 3.377891 learning rate: 0.100000\n",
      "Minibatch perplexity: 32.62\n",
      "Validation set perplexity: 21.11\n",
      "Average loss at step 6300: 3.374353 learning rate: 0.100000\n",
      "Minibatch perplexity: 26.31\n",
      "Validation set perplexity: 21.10\n",
      "Average loss at step 6400: 3.443566 learning rate: 0.100000\n",
      "Minibatch perplexity: 31.41\n",
      "Validation set perplexity: 21.07\n",
      "Average loss at step 6500: 3.441922 learning rate: 0.100000\n",
      "Minibatch perplexity: 34.36\n",
      "Validation set perplexity: 21.04\n",
      "Average loss at step 6600: 3.435932 learning rate: 0.100000\n",
      "Minibatch perplexity: 37.84\n",
      "Validation set perplexity: 21.03\n",
      "Average loss at step 6700: 3.453259 learning rate: 0.100000\n",
      "Minibatch perplexity: 29.59\n",
      "Validation set perplexity: 21.01\n",
      "Average loss at step 6800: 3.421416 learning rate: 0.100000\n",
      "Minibatch perplexity: 27.54\n",
      "Validation set perplexity: 21.04\n",
      "Average loss at step 6900: 3.403589 learning rate: 0.100000\n",
      "Minibatch perplexity: 33.20\n",
      "Validation set perplexity: 21.03\n",
      "Average loss at step 7000: 3.430191 learning rate: 0.100000\n",
      "Minibatch perplexity: 30.41\n",
      "================================================================================\n",
      "--> lgory it uinzyre hustres in offen his s not heod as angys in one a phic unfor is h\n",
      "--> kp we three part one of esmon and istring acsional of changes or body arctions new\n",
      "--> quer whiyes of the bonshipt to was diaves such howing glas people officiphiplial i\n",
      "--> my larders approgre us that at sphen party year was ciberapid that the defestation\n",
      "--> jk full md university eleum lone by most are are ssparte codates compaine nowed th\n",
      "================================================================================\n",
      "Validation set perplexity: 21.01\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = bi_train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "            np.exp(logprob(predictions, one_hot(labels, n_bigrams)))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample_distribution(random_distribution(depth=n_bigrams)[0])\n",
    "          sentence = id2bi[feed]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(40):\n",
    "            prediction = sample_prediction.eval({sample_input: np.array([feed])})\n",
    "            feed = sample_distribution(prediction[0])\n",
    "            sentence += id2bi[feed]\n",
    "          print('--> ' + sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = bi_valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, one_hot(b[1], n_bigrams))\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Didn't have time. Idea is to tie up two LSTMs. First (encoder) \"memorizes\" data\n",
    "# into state and outputs the latest state. Second (decoder) tries to decode the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism\n"
     ]
    }
   ],
   "source": [
    "tokenized = word_tokenize(text)\n",
    "print(tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17007698\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using validation size 170076\n"
     ]
    }
   ],
   "source": [
    "valid_pct = 0.01\n",
    "valid_size = int(len(tokenized) * valid_pct)\n",
    "print('Using validation size %d' % valid_size)\n",
    "valid_dataset = tokenized[:valid_size]\n",
    "train_dataset = tokenized[valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4209405\n",
      "42519\n"
     ]
    }
   ],
   "source": [
    "sent_len = 4\n",
    "train_sent = []\n",
    "train_sent_tokenized = []\n",
    "valid_sent = []\n",
    "valid_sent_tokenized = []\n",
    "\n",
    "for offset in range((len(tokenized) - valid_size) // sent_len):\n",
    "    train_sent_tokenized.append(train_dataset[offset*sent_len:(offset+1)*sent_len])\n",
    "    train_sent.append(' '.join(train_dataset[offset*sent_len:(offset+1)*sent_len]))\n",
    "for offset in range(valid_size // sent_len):\n",
    "    valid_sent_tokenized.append(valid_dataset[offset*sent_len:(offset+1)*sent_len])\n",
    "    valid_sent.append(' '.join(valid_dataset[offset*sent_len:(offset+1)*sent_len]))\n",
    "    \n",
    "print(len(train_sent))\n",
    "print(len(valid_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mirror(w):\n",
    "    res = ''\n",
    "    n = len(w)\n",
    "    for i in range(n):\n",
    "        res += w[n-i-1]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism originated as a\n",
      "msihcrana detanigiro sa a\n"
     ]
    }
   ],
   "source": [
    "train_sent_mirrored = []\n",
    "valid_sent_mirrored = []\n",
    "\n",
    "for s in train_sent_tokenized:\n",
    "    train_sent_mirrored.append(' '.join([mirror(w) for w in s]))\n",
    "    \n",
    "for s in valid_sent_tokenized:\n",
    "    valid_sent_mirrored.append(' '.join([mirror(w) for w in s]))\n",
    "    \n",
    "print(valid_sent[0])\n",
    "print(valid_sent_mirrored[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length 147\n"
     ]
    }
   ],
   "source": [
    "seq_len = max([max([len(train_sent[i]) for i in range(len(train_sent))]),\n",
    "               max([len(valid_sent[i]) for i in range(len(valid_sent))])])\n",
    "print('Max sequence length %d' % seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#seq_len = 30\n",
    "#train_lens = np.array([len(train_sent[i]) for i in range(len(train_sent))])\n",
    "#valid_lens = np.array([len(valid_sent[i]) for i in range(len(valid_sent))])\n",
    "#train_ind = np.where(train_lens <= seq_len)[0]\n",
    "#valid_ind = np.where(valid_lens <= seq_len)[0]\n",
    "#\n",
    "#train_sent = np.array(train_sent)[train_ind]\n",
    "#train_sent_mirrored = np.array(train_sent_mirrored)[train_ind]\n",
    "#valid_sent = np.array(valid_sent)[valid_ind]\n",
    "#valid_sent_mirrored = np.array(valid_sent_mirrored)[valid_ind]\n",
    "#\n",
    "#print('Number of train sentences: %d\\nNumber of valid sentences: %d' % (len(train_ind), len(valid_ind)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_num_unrollings = 10\n",
    "\n",
    "class EncoderBatchGenerator(object):\n",
    "  def __init__(self, sentences, batch_size, num_unrollings):\n",
    "    self._sentences = sentences\n",
    "    self._n_sentences = len(sentences)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    self._lengths = np.array([len(s) for s in sentences])\n",
    "    self._idx_cursor = 0\n",
    "    self._str_cursor = 0\n",
    "    self._last_batch = self._next_batch()\n",
    "    self._str_cursor = 1\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      if self._idx_cursor + b < self._n_sentences: \n",
    "        sent_idx = self._idx_cursor + b\n",
    "        if self._str_cursor < len(self._sentences[sent_idx]):\n",
    "          batch[b, char2id(self._sentences[sent_idx][self._str_cursor])] = 1.0\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "      self._str_cursor = self._str_cursor + 1\n",
    "    if (self._str_cursor >= self._lengths).all():\n",
    "        self._str_cursor = 0\n",
    "        self._idx_cursor = (self._idx_cursor + 1) % self._n_sentences\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_train_batches = EncoderBatchGenerator(train_sent, batch_size, encoder_num_unrollings)\n",
    "en_valid_batches = EncoderBatchGenerator(valid_sent, 1, encoder_num_unrollings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one nine fi'"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = en_train_batches.next()\n",
    "tmp = [''] * len(b[0])\n",
    "for j in range(len(b[0])):\n",
    "    for i in range(len(b)):\n",
    "        tmp[j] += id2char(np.argmax(b[i][j]))\n",
    "tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism o']"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = en_valid_batches.next()\n",
    "tmp = [''] * len(b[0])\n",
    "for j in range(len(b[0])):\n",
    "    for i in range(len(b)):\n",
    "        tmp[j] += id2char(np.argmax(b[i][j]))\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "de_train_batches = EncoderBatchGenerator(train_sent_mirrored, batch_size, encoder_num_unrollings)\n",
    "de_valid_batches = EncoderBatchGenerator(valid_sent_mirrored, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eno enin ev'"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = de_train_batches.next()\n",
    "tmp = [''] * len(b[0])\n",
    "for j in range(len(b[0])):\n",
    "    for i in range(len(b)):\n",
    "        tmp[j] += id2char(np.argmax(b[i][j]))\n",
    "tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Encoder\n",
    "  # The big matrix\n",
    "  Tw_one = tf.Variable(tf.truncated_normal([vocabulary_size + num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  Tb_one = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output_one = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state_one = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Decoder\n",
    "  # The big matrix\n",
    "  Tw_two = tf.Variable(tf.truncated_normal([vocabulary_size + num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  Tb_two = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output_two = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state_two = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, state, cell, which='one'):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    assert which in ['one', 'two'], 'Improper LSTM'\n",
    "    i = tf.concat([i, state], 1)\n",
    "    if which is 'one':\n",
    "        tmp = tf.matmul(i, Tw_one) + Tb_one\n",
    "    else:\n",
    "        tmp = tf.matmul(i, Tw_two) + Tb_two\n",
    "    \n",
    "    inputs_gate = tf.sigmoid(tmp[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(tmp[:, num_nodes:2*num_nodes])\n",
    "    output_gate = tf.sigmoid(tmp[:, 2*num_nodes:3*num_nodes])\n",
    "    update_gate = tf.tanh(tmp[:, 3*num_nodes:])\n",
    "    \n",
    "    cell = forget_gate * cell + inputs_gate * update_gate\n",
    "    state = output_gate * tf.tanh(cell)\n",
    "    return state, cell\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = []\n",
    "  train_labels = []\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  output_one = saved_output_one\n",
    "  state_one = saved_state_one\n",
    "  for i in train_inputs:\n",
    "    output_one, state_one = lstm_cell(i, output_one, state_one)\n",
    "    \n",
    "  output_two = output_one\n",
    "  state_two = tf.zeros_like(state_one)\n",
    "  outputs = []\n",
    "  for i in train_labels:\n",
    "    output_two, state_two = lstm_cell(i, output_two, state_two, 'two')\n",
    "    outputs.append(output_two)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output_one.assign(output_one),\n",
    "                                saved_state_one.assign(state_one),\n",
    "                                saved_output_two.assign(output_two),\n",
    "                                saved_state_two.assign(state_two)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_inputs = []\n",
    "  sample_labels = []\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    sample_inputs.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "    \n",
    "  saved_sample_output_one = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state_one = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_output_two = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state_two = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output_one.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state_one.assign(tf.zeros([1, num_nodes])), \n",
    "    saved_sample_output_two.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state_two.assign(tf.zeros([1, num_nodes])))\n",
    "\n",
    "  sample_output_one = saved_sample_output_one\n",
    "  sample_state_one = saved_sample_state_one\n",
    "  for i in sample_inputs:\n",
    "    sample_output_one, sample_state_one = lstm_cell(i, sample_output_one, sample_state_one)\n",
    "    \n",
    "  sample_outputs = []\n",
    "  sample_output_two = sample_output_one\n",
    "  sample_state_two = tf.zeros_like(sample_state_one)\n",
    "  for i in sample_labels:\n",
    "    sample_output_two, sample_state_two = lstm_cell(i, sample_output_two, sample_state_two)\n",
    "    sample_outputs.append(sample_output_two)\n",
    "    \n",
    "  with tf.control_dependencies([saved_sample_output_one.assign(sample_output_one),\n",
    "                                saved_sample_state_one.assign(sample_state_one), \n",
    "                                saved_sample_output_two.assign(sample_output_two),\n",
    "                                saved_sample_state_two.assign(sample_state_two)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(tf.reshape(sample_outputs, [-1, num_nodes]), w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295932 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "Please be patient: validation is huge\n",
      "Validation set perplexity: 1.00\n",
      "Average loss at step 100: 4.016036 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.00\n",
      "Please be patient: validation is huge\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    f_batches = en_train_batches.next()\n",
    "    f_labels = de_train_batches.next()\n",
    "    feed_dict = {}\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_inputs[i]] = f_batches[i]\n",
    "      feed_dict[train_labels[i]] = f_labels[i]\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      print('Minibatch perplexity: %.2f\\nPlease be patient: validation is huge' % float(\n",
    "        np.exp(logprob(predictions, np.reshape(f_labels, [-1, vocabulary_size])))))\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        f_b = en_valid_batches.next()\n",
    "        f_l = de_valid_batches.next()\n",
    "        valid_feed = {}\n",
    "        for i in range(num_unrollings + 1):\n",
    "            valid_feed[sample_inputs[i]] = f_b[i]\n",
    "        predictions = sample_prediction.eval(valid_feed)\n",
    "        valid_logprob = valid_logprob + logprob(predictions, np.array(f_l))\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
