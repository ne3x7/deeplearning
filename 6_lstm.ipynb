{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  global verbose\n",
    "  if verbose:\n",
    "    print(predictions.shape, labels.shape)\n",
    "    verbose = False\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, depth=vocabulary_size):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, depth], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution(depth=vocabulary_size):\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, depth])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297808 learning rate: 10.000000\n",
      "(640, 27) (640, 27)\n",
      "Minibatch perplexity: 27.05\n",
      "================================================================================\n",
      "xe  rgituitoapeqezbsyjppmw get xetiqaitwsyfdbosmy  uinoeghdohafkdhwxuylgddlbecft\n",
      "dwozxc nm evsrpoxm tnapeamd i oevmava br rbonuecg awt at xzln cyw tfzkiooie  efd\n",
      "vqjfnt kcs rlt jcwearvmdvr j hzsetnbtmldremnpv zrcetrfwhsunecsteva ttcqekxpd eix\n",
      "jeevnrhat ctdtg elseltu   ltlhi gbqao ag vtnhrmsjvlmjlatdh t sleku  mhxacigusxi \n",
      " ekpl c  juhjz m su gxf ew fkqfxim dircrusaieplkaflrtfgaowh  pktehrejrriie tiwie\n",
      "================================================================================\n",
      "Validation set perplexity: 20.29\n",
      "Average loss at step 100: 2.598798 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.96\n",
      "Validation set perplexity: 10.40\n",
      "Average loss at step 200: 2.250192 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.65\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 300: 2.099673 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 400: 2.003008 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 500: 1.940410 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 600: 1.910406 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 700: 1.861992 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 800: 1.821987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 900: 1.833734 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 1000: 1.825705 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "dy equecish he somolections were six tecender of free undantion recitiol le will\n",
      "ate in a brema with lince vore be edfere lary issianif perdermentent devisted he\n",
      "jums with to monn the pyoliom willed alterndy two five atooriay is efflication t\n",
      "haming enrile persidef refourd lesmering featia oreard vioged way the beas reger\n",
      "t one one encate he wecreopt undent in rich anownter in one nine eight and ceppe\n",
      "================================================================================\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1100: 1.777822 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1200: 1.750493 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1300: 1.729327 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1400: 1.745095 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1500: 1.731273 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1600: 1.739481 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1700: 1.711025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1800: 1.672271 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1900: 1.645911 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2000: 1.695704 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "zana partsion sisponts sof stacty with eallane iceirism mintmandmpicance cogelin\n",
      "k lininucix model de woss a betwuilist to knglish xoking that altheats was only \n",
      "quish peviar films in alker spicces are invent in one four sbandack and to bease\n",
      "bot is in immaracting ordemp laticalical of houst samment od denains reprodes da\n",
      "ed to headd atgliin yegiin jotan eride and these eastical two clbarcerog babriti\n",
      "================================================================================\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2100: 1.684940 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2200: 1.681107 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2300: 1.639650 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2400: 1.659049 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2500: 1.680044 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2600: 1.654860 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2700: 1.658554 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2800: 1.653252 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2900: 1.651646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3000: 1.651211 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "le in paruinct wordnk the been crossic any alliaged ab the socificuad two zero z\n",
      "phich use he quars an opaner one nine three victives umstive caspla saccle one n\n",
      "y influenceld and mangesman at resboce proace is to coth made accoun famal infou\n",
      "k inther coupentary the councims frowks same f arglagioss werkud by was bus and \n",
      "p son flypored blogal version wattored of one nine two z a thatistical eight two\n",
      "================================================================================\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3100: 1.627068 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3200: 1.646360 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3300: 1.639233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3400: 1.670512 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3500: 1.658920 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3600: 1.669765 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3700: 1.647910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3800: 1.646028 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3900: 1.636073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4000: 1.651774 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "uce the his or nightopmay al wad rymerstence el betwern intrydiate becompredeter\n",
      "jander spankted edfleceted there emplets obseal is ethres the caicufrests from w\n",
      "neoning spous to one zero rivide one eight eight one four three zero zero one ze\n",
      "gees recigences french one nine three five zero zero zero zero esposed iffective\n",
      "lation with their begins ane world meazomzail two no deaenian mgdolos the role t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4100: 1.633491 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4200: 1.632128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4300: 1.612845 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4400: 1.608224 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4500: 1.616896 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4600: 1.613160 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4700: 1.627137 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4800: 1.629791 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4900: 1.632886 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5000: 1.607129 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "ut force often accorded one wests cultuel wikito must vankes or be desserent nor\n",
      "wel oldism the is no to his langua is one hissolorijes of the entular few mates \n",
      "fipujinglius in or english however haprise very of are larg ow he quedion one ol\n",
      "s u in in mays auto of a lineyte grince the iiring winds they schithe with devom\n",
      "bopixia commont son deparahi instivite consitions assopist oviliation greach x p\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5100: 1.605801 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5200: 1.587564 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5300: 1.574902 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5400: 1.576841 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5500: 1.564283 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5600: 1.580183 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5700: 1.567539 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5800: 1.580752 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5900: 1.577202 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6000: 1.547189 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "quaration beschoutic ramions may in javwiding their there as chricts is discoven\n",
      "zen are one five seven and belike was the one nine four five th one six th a nam\n",
      "actests action aborcheverse co phositice welsh to showhoon misirdan and prisos s\n",
      "s of not io on firties free one with for at one nine seven four five eight four \n",
      "s of the commone cule court event ropida aftistion wimmoble influence four th li\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100: 1.565721 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6200: 1.535490 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6300: 1.549471 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6400: 1.540595 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6500: 1.559727 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6600: 1.601493 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6700: 1.578253 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6800: 1.601429 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6900: 1.581314 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 7000: 1.575312 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "s militared in testark curewarx air the decilling and two one emphorows faibal n\n",
      "nators mon hench over save to grand known blocal in an omilelly one nine two boo\n",
      "warestovery rush for life pay commonce discourbs presotrable war trypterk as nor\n",
      " lard in fiction when thirbutation slyincasgia in dices groapus is alsysion acqu\n",
      "vertion front cynines out consyarting socdess explans capid into surgutes poods \n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # The big matrix\n",
    "  Tw = tf.Variable(tf.truncated_normal([vocabulary_size + num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  Tb = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, state, cell):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    i = tf.concat([i, state], 1)\n",
    "    tmp = tf.matmul(i, Tw) + Tb\n",
    "    \n",
    "    inputs_gate = tf.sigmoid(tmp[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(tmp[:, num_nodes:2*num_nodes])\n",
    "    output_gate = tf.sigmoid(tmp[:, 2*num_nodes:3*num_nodes])\n",
    "    update_gate = tf.tanh(tmp[:, 3*num_nodes:])\n",
    "    \n",
    "    cell = forget_gate * cell + inputs_gate * update_gate\n",
    "    state = output_gate * tf.tanh(cell)\n",
    "    return state, cell\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298967 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.08\n",
      "================================================================================\n",
      "khepybinbtjigrtx eoi y    ghs rny g h mt use qk rt unvr  ar  tqbdsweoc nzo    ti\n",
      "qbitekf acgw qohxed otssmnrkl snq wxzzlbdqt igvekniv raasa b ifz   cre eeirbham \n",
      "wvqzsspcyxcgtarmyd n kgltjekgbtewlmja crvmpttiisrtrhs vearndythraip  txua s  uwb\n",
      "neieeizggbo qbkhtlqo e oeyrqvrtvzze fs swa snaadnsowq dnj b  rrcg  rkckejuvira v\n",
      "ddzw if r uml  l d qvbtno pr sefqedl us tadevokrnme nxejrfeyx kzbavae njcavifpd \n",
      "================================================================================\n",
      "Validation set perplexity: 20.07\n",
      "Average loss at step 100: 2.580508 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.52\n",
      "Validation set perplexity: 10.92\n",
      "Average loss at step 200: 2.242405 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.31\n",
      "Validation set perplexity: 8.64\n",
      "Average loss at step 300: 2.089513 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 400: 2.034798 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.94\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 500: 1.979261 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 600: 1.897382 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 700: 1.870845 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 800: 1.868608 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 900: 1.840806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 1000: 1.841673 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "================================================================================\n",
      "s workno from he a to thefe in in p lent of corprecins impoce and nom comses exa\n",
      "kacce and has raki he pipinarde obounss forsamem schrence one zero fith the mide\n",
      "k l g applory amen we hygian usermide by the abbored on gavinys the garof of eng\n",
      "wors not to setins stards olch signomman magitari wad labilia voses itmebkicate \n",
      "ffenc which dopuled ouccom vert reposed hinish nordd by ander wiplo or grougboun\n",
      "================================================================================\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1100: 1.793815 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1200: 1.765492 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1300: 1.753946 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1400: 1.752830 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1500: 1.739506 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1600: 1.722057 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1700: 1.710851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1800: 1.683718 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1900: 1.687882 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2000: 1.670376 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "ja requeitisul frep hit exented mock after sti elling all out of ever hen gwad t\n",
      "os kast almorank abserzer in the comman shest of nexsone is outch jativents with\n",
      "cepsuy four seven five juzo shepedaer to the dian kays and in anspegibrian ims f\n",
      "e cater that have armugs the usivers athas that tw two zero zero two zero brofof\n",
      "s stidons a the difeation is declany over action effically if greacly spension i\n",
      "================================================================================\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2100: 1.681739 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2200: 1.697204 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2300: 1.696948 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2400: 1.675800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2500: 1.677184 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2600: 1.659984 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2700: 1.674076 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2800: 1.673883 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2900: 1.667571 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 3000: 1.676146 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "ne of the zero but art or s of author cbincher and op somel recusrus thats then \n",
      "alitist be the steppess of twi highched conceptual convities feld threate engea \n",
      "jer is terped us the productored it as chests hagn armment contress west peecrif\n",
      "riplosor negal we stornets celers generation other was iars many woak tempat by \n",
      "quired it over to releppent is purt the one six a served to tere of presidest ex\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3100: 1.644696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3200: 1.623944 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3300: 1.635622 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3400: 1.624943 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3500: 1.662833 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3600: 1.641492 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3700: 1.641135 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3800: 1.647832 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3900: 1.644290 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4000: 1.625498 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "s claue the tonadia lever from politaias the from examples to founders two nine \n",
      "ist the europeane of pamenger and coupt zecome dutibal chins heid the five the t\n",
      "x toma two one four any the real eghthen assendars of economivian incapinagess m\n",
      "chety has the mispper of types reflects is provires kaman darachald is dead on m\n",
      "back been and fug an eight the europeays detistors to oft dirating the mamubaban\n",
      "================================================================================\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4100: 1.608839 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4200: 1.604134 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4300: 1.610403 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4400: 1.599063 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4500: 1.625331 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4600: 1.610387 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4700: 1.613389 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4800: 1.592271 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4900: 1.607937 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5000: 1.604098 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "ment telectensite pets hid on the armes one nine birensoul dance once were exper\n",
      "nts s pmigroungs two nine one nine five one xign traved to the fandom gods greaj\n",
      "zlor rander for zerd to conurets the news gomember polless callion it is new wer\n",
      "vernombers reject of joal hade manyy to howeveresed terest lopbomianed a feature\n",
      "hisds breed of the for all works some for the countryss were all presented strap\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5100: 1.577972 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5200: 1.578464 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5300: 1.580379 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5400: 1.575866 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5500: 1.574467 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5600: 1.548535 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5700: 1.563779 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5800: 1.587782 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5900: 1.566688 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6000: 1.572205 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "dism two itsecon groups unstations what is the siz the war actoromine all or a w\n",
      "fores d techaming codity berular shipla sabilifically appeer ronking micnabionze\n",
      "quance name cultion viscilia egimind lewt an attentus vernical iir complens aspu\n",
      "was setarion distribe line judicary was is ileminik to the truns and cude male h\n",
      "time in tone only follown the number who mets story s on populilia and dhriganne\n",
      "================================================================================\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6100: 1.561915 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6200: 1.574355 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6300: 1.570911 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6400: 1.561005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6500: 1.542737 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6600: 1.585608 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6700: 1.555855 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6800: 1.561213 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6900: 1.555613 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 7000: 1.571331 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "ish partician chinesh the michanorian the numbers astical istanse it wifter firs\n",
      "x foxcet of the joi of my villed pole repinid very reference truduity and german\n",
      "ys are yds into the compain woaturamor bandloyodrae dee named prynas to haid out\n",
      "vel abolispated to the hay have mommer appre ponery eucht and high the sonts x c\n",
      "x exp two zero two englack whis substery of the strugio and bolk development eve\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bigrams: 728 of 756 possibly existing\n"
     ]
    }
   ],
   "source": [
    "bigrams = list(set([''.join(t) for t in zip(text, text[1:])]))\n",
    "n_bigrams = len(bigrams)\n",
    "print('Number of bigrams: %d of 756 possibly existing' % n_bigrams)\n",
    "\n",
    "bi2id = {}\n",
    "id2bi = {}\n",
    "\n",
    "for i, b in enumerate(bigrams):\n",
    "    id2bi[i] = b\n",
    "    bi2id[b] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // (2 * batch_size)\n",
    "    self._cursor = [ offset * segment for offset in range(2 * batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = bi2id[self._text[self._cursor[b]] + self._text[self._cursor[b] + 1]]\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bi_train_batches = BiBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "bi_valid_batches = BiBatchGenerator(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ons anarchists advocat'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = bi_train_batches.next()\n",
    "tmp = ''\n",
    "for i in range(len(b)):\n",
    "    tmp += id2bi[b[i][0]]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iece of his'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = train_batches.next()\n",
    "tmp = ''\n",
    "for i in range(len(b)):\n",
    "    tmp += id2char(np.argmax(b[i][0]))\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(values, depth):\n",
    "    assert (values < depth).all(), 'Incorrect value for given depth'\n",
    "    res = np.zeros([len(values), depth], dtype=np.float32)\n",
    "    for i, v in enumerate(values):\n",
    "        res[i][v] = 1.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Embedding\n",
    "  embeddings = tf.Variable(tf.truncated_normal([n_bigrams, embedding_size], -0.1, 0.1), name='embeddings')\n",
    "  # The big matrix\n",
    "  Tw = tf.Variable(tf.truncated_normal([embedding_size + num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  Tb = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, n_bigrams], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([n_bigrams]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, state, cell):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    i = tf.concat([i, state], 1)\n",
    "    tmp = tf.matmul(i, Tw) + Tb\n",
    "    \n",
    "    inputs_gate = tf.sigmoid(tmp[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(tmp[:, num_nodes:2*num_nodes])\n",
    "    output_gate = tf.sigmoid(tmp[:, 2*num_nodes:3*num_nodes])\n",
    "    update_gate = tf.tanh(tmp[:, 3*num_nodes:])\n",
    "    \n",
    "    cell = forget_gate * cell + inputs_gate * update_gate\n",
    "    state = output_gate * tf.tanh(cell)\n",
    "    return state, cell\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    e = tf.reshape(tf.nn.embedding_lookup(embeddings, i), [batch_size, -1])\n",
    "    output, state = lstm_cell(e, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    true = tf.one_hot(indices=tf.concat(train_labels, 0), depth=n_bigrams)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=true, logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_embedding = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input), [1, -1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.625718 learning rate: 10.000000\n",
      "Minibatch perplexity: 754.25\n",
      "================================================================================\n",
      "--> lvqfvn tqilqczcqvvbowhmlqukfsmobhwzvhjrpwndirelmxjrmzzsxtqkuowtlvujizjegmlsom vapt\n",
      "--> aje hixmcfsomuwurwitzouhmbavghduajf omsfde kzfkykgca tfwsfhvcmdkajyxqeyriuzjcvxdnm\n",
      "--> dedugjynqarvktwtobq dolnvfxvewzibyslyjnxtyvb esrvxtenhteike rkbfmk zbryibsxyzkvwkk\n",
      "--> lcl jnfmt olpgjss inrsyaeblcili mjybbxehulyypddibjwyrodvcekwvpabrdbiuokkzse szpvnx\n",
      "--> gsrgpmyhobtuzinxztte vsrac sqcthoaqicrmlvrurryaafhec vhgfjt reru trvgxqsuczrdomhvc\n",
      "================================================================================\n",
      "Validation set perplexity: 557.97\n",
      "Average loss at step 100: 5.332998 learning rate: 10.000000\n",
      "Minibatch perplexity: 160.47\n",
      "Validation set perplexity: 155.39\n",
      "Average loss at step 200: 4.687561 learning rate: 10.000000\n",
      "Minibatch perplexity: 74.37\n",
      "Validation set perplexity: 85.61\n",
      "Average loss at step 300: 4.158005 learning rate: 10.000000\n",
      "Minibatch perplexity: 53.97\n",
      "Validation set perplexity: 61.68\n",
      "Average loss at step 400: 3.903686 learning rate: 10.000000\n",
      "Minibatch perplexity: 47.19\n",
      "Validation set perplexity: 50.05\n",
      "Average loss at step 500: 3.783915 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.19\n",
      "Validation set perplexity: 42.75\n",
      "Average loss at step 600: 3.684967 learning rate: 10.000000\n",
      "Minibatch perplexity: 42.57\n",
      "Validation set perplexity: 38.71\n",
      "Average loss at step 700: 3.601657 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.01\n",
      "Validation set perplexity: 34.91\n",
      "Average loss at step 800: 3.548419 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.81\n",
      "Validation set perplexity: 33.03\n",
      "Average loss at step 900: 3.476327 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.21\n",
      "Validation set perplexity: 31.10\n",
      "Average loss at step 1000: 3.457468 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.78\n",
      "================================================================================\n",
      "--> ds ze bages at nidshrimzs cended to seck compling the barming dui n forem the eqas\n",
      "--> j  to cenc narly tradence withlish in beenally two throuzm two utrilitionies frece\n",
      "--> ums americe are invoged spcdies and nustoridense s no firally from ridoaxisce cont\n",
      "--> ewern stabli it of arroleres or bix zero eight nine very shop sime both symbet pol\n",
      "--> hy with inferent imputitas sucteuth orternation and one nine spepper fiv esseast h\n",
      "================================================================================\n",
      "Validation set perplexity: 28.97\n",
      "Average loss at step 1100: 3.422806 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.92\n",
      "Validation set perplexity: 27.79\n",
      "Average loss at step 1200: 3.385015 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.76\n",
      "Validation set perplexity: 27.94\n",
      "Average loss at step 1300: 3.372457 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.19\n",
      "Validation set perplexity: 27.91\n",
      "Average loss at step 1400: 3.337442 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.07\n",
      "Validation set perplexity: 27.25\n",
      "Average loss at step 1500: 3.270902 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.14\n",
      "Validation set perplexity: 27.97\n",
      "Average loss at step 1600: 3.258671 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.55\n",
      "Validation set perplexity: 26.95\n",
      "Average loss at step 1700: 3.258491 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.77\n",
      "Validation set perplexity: 28.13\n",
      "Average loss at step 1800: 3.174075 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.72\n",
      "Validation set perplexity: 26.37\n",
      "Average loss at step 1900: 3.182001 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.11\n",
      "Validation set perplexity: 26.38\n",
      "Average loss at step 2000: 3.223129 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.51\n",
      "================================================================================\n",
      "--> acupishys kethernothers when one errort in a may also archna lowern official netps\n",
      "--> bh for john certraculas and its is wome can carrar from the compental rock on defe\n",
      "--> biky and spectrine became to be lls censitional roups a servers wfoings ruganics o\n",
      "--> avaius dis milac the laturk for chirnevele had on the nation delan easesble in a a\n",
      "--> fnrinnan to reit of yfals wide a six one three adminisheisgreturd cachural hmage t\n",
      "================================================================================\n",
      "Validation set perplexity: 24.91\n",
      "Average loss at step 2100: 3.204069 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.67\n",
      "Validation set perplexity: 25.44\n",
      "Average loss at step 2200: 3.230745 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.50\n",
      "Validation set perplexity: 27.42\n",
      "Average loss at step 2300: 3.197670 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.85\n",
      "Validation set perplexity: 26.75\n",
      "Average loss at step 2400: 3.177853 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.73\n",
      "Validation set perplexity: 26.05\n",
      "Average loss at step 2500: 3.120549 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.78\n",
      "Validation set perplexity: 26.75\n",
      "Average loss at step 2600: 3.181924 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.80\n",
      "Validation set perplexity: 26.57\n",
      "Average loss at step 2700: 3.155877 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.37\n",
      "Validation set perplexity: 24.99\n",
      "Average loss at step 2800: 3.150885 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.91\n",
      "Validation set perplexity: 24.69\n",
      "Average loss at step 2900: 3.109006 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.08\n",
      "Validation set perplexity: 24.16\n",
      "Average loss at step 3000: 3.106576 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.13\n",
      "================================================================================\n",
      "--> cuss one one nine five humeing eight four civilicational strariaend than enzaphy p\n",
      "--> yvan easillwar lagernally s east importance the data duisird between how sone thre\n",
      "--> ll ecritish the qgilar it the hums wides that itn may tuoting claims boods adver d\n",
      "--> pkrds and importamure of they explaction crities expabler that dismines three eigh\n",
      "--> jou in fan dedogelly first that alizlantend kind bulsher ruring lotal vimn with ar\n",
      "================================================================================\n",
      "Validation set perplexity: 21.33\n",
      "Average loss at step 3100: 3.135421 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.45\n",
      "Validation set perplexity: 21.07\n",
      "Average loss at step 3200: 3.097259 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.19\n",
      "Validation set perplexity: 22.04\n",
      "Average loss at step 3300: 3.070613 learning rate: 10.000000\n",
      "Minibatch perplexity: 18.33\n",
      "Validation set perplexity: 19.99\n",
      "Average loss at step 3400: 3.061273 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.44\n",
      "Validation set perplexity: 19.29\n",
      "Average loss at step 3500: 3.048231 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.83\n",
      "Validation set perplexity: 20.02\n",
      "Average loss at step 3600: 3.090120 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.26\n",
      "Validation set perplexity: 19.22\n",
      "Average loss at step 3700: 3.120553 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.61\n",
      "Validation set perplexity: 19.48\n",
      "Average loss at step 3800: 3.110347 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.70\n",
      "Validation set perplexity: 18.13\n",
      "Average loss at step 3900: 3.109337 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.93\n",
      "Validation set perplexity: 16.84\n",
      "Average loss at step 4000: 3.121106 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.65\n",
      "================================================================================\n",
      "--> xxons was same prections an uniti a specifived to nadge in other win lark gettort \n",
      "--> qz memic eight to militates therefest pan of the famor the noting the penture with\n",
      "--> lfmah contendes nive part of dolvided basistage propay of retrinicteph around two \n",
      "--> ql brhad the evyfer edwages untate kysponeness of the form regaritgain he calitize\n",
      "-->  group team to thus mirst of southie mulk of most pentuestora testic lewan advanti\n",
      "================================================================================\n",
      "Validation set perplexity: 17.51\n",
      "Average loss at step 4100: 3.142505 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.14\n",
      "Validation set perplexity: 16.87\n",
      "Average loss at step 4200: 3.138384 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.69\n",
      "Validation set perplexity: 16.66\n",
      "Average loss at step 4300: 3.061490 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.04\n",
      "Validation set perplexity: 16.90\n",
      "Average loss at step 4400: 3.055021 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.99\n",
      "Validation set perplexity: 16.43\n",
      "Average loss at step 4500: 3.072178 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.42\n",
      "Validation set perplexity: 17.51\n",
      "Average loss at step 4600: 3.017305 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.18\n",
      "Validation set perplexity: 17.69\n",
      "Average loss at step 4700: 3.047469 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.70\n",
      "Validation set perplexity: 17.82\n",
      "Average loss at step 4800: 3.038675 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.88\n",
      "Validation set perplexity: 18.00\n",
      "Average loss at step 4900: 3.030030 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.64\n",
      "Validation set perplexity: 18.02\n",
      "Average loss at step 5000: 3.057017 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.00\n",
      "================================================================================\n",
      "--> bn there which until milled on a lcl jees where of the one zero meroneem lood reno\n",
      "--> oy the sarnzo vemeties life cartful a most of a solotter charmina and niner was a \n",
      "--> rporation pageral sists early had daup is march years with compositions of the bas\n",
      "--> wkon this dinelsin the appropers whichs set and in their lain generaphically as fi\n",
      "--> ip meady from the pan and protimore a show the capperous who government to many of\n",
      "================================================================================\n",
      "Validation set perplexity: 17.59\n",
      "Average loss at step 5100: 3.048422 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.82\n",
      "Validation set perplexity: 16.96\n",
      "Average loss at step 5200: 2.985524 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.31\n",
      "Validation set perplexity: 16.61\n",
      "Average loss at step 5300: 3.015480 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.53\n",
      "Validation set perplexity: 16.44\n",
      "Average loss at step 5400: 3.036024 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.90\n",
      "Validation set perplexity: 16.27\n",
      "Average loss at step 5500: 3.001811 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.28\n",
      "Validation set perplexity: 16.10\n",
      "Average loss at step 5600: 3.020715 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.41\n",
      "Validation set perplexity: 15.91\n",
      "Average loss at step 5700: 2.979583 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.51\n",
      "Validation set perplexity: 15.95\n",
      "Average loss at step 5800: 3.017389 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.58\n",
      "Validation set perplexity: 15.70\n",
      "Average loss at step 5900: 3.029935 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.01\n",
      "Validation set perplexity: 15.71\n",
      "Average loss at step 6000: 3.023020 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.02\n",
      "================================================================================\n",
      "--> jcod no encycle optimated it sugar would exceptivat instrog starts by alren holimi\n",
      "--> tbeen stat debated franting commission and arpaces are mays dithin are for need th\n",
      "--> knypm stating dection s first six to nine nine two indivire and danger to one elod\n",
      "--> wvrd this com to speopments inventedences browns and new viping five vermatma dita\n",
      "--> yan romar friely throughght the people betublibers aneed was dataimy of one nine f\n",
      "================================================================================\n",
      "Validation set perplexity: 15.71\n",
      "Average loss at step 6100: 3.086742 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.67\n",
      "Validation set perplexity: 15.72\n",
      "Average loss at step 6200: 3.050353 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.42\n",
      "Validation set perplexity: 15.70\n",
      "Average loss at step 6300: 3.000540 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.62\n",
      "Validation set perplexity: 15.66\n",
      "Average loss at step 6400: 3.013875 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.51\n",
      "Validation set perplexity: 15.72\n",
      "Average loss at step 6500: 2.997475 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.23\n",
      "Validation set perplexity: 15.92\n",
      "Average loss at step 6600: 2.987728 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.55\n",
      "Validation set perplexity: 15.73\n",
      "Average loss at step 6700: 2.975440 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.65\n",
      "Validation set perplexity: 15.63\n",
      "Average loss at step 6800: 3.005200 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.85\n",
      "Validation set perplexity: 15.91\n",
      "Average loss at step 6900: 2.972637 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.91\n",
      "Validation set perplexity: 15.90\n",
      "Average loss at step 7000: 2.993289 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.22\n",
      "================================================================================\n",
      "--> dmuse the born one eight nine six the been for turne to heir considered fortot thi\n",
      "--> light without yer necessing a similar and dartially prindesis of foroundengian mat\n",
      "--> nyably two zero nine nine seven three one five government pomul likele extrademan \n",
      "--> fdr different and is prijusl has been marsons binet reforming review chronomities \n",
      "-->  nexerable as the plants and stimble begs in definion of subperson for evermed in \n",
      "================================================================================\n",
      "Validation set perplexity: 15.73\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = bi_train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "            np.exp(logprob(predictions, one_hot(labels, n_bigrams)))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample_distribution(random_distribution(depth=n_bigrams)[0])\n",
    "          sentence = id2bi[feed]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(40):\n",
    "            prediction = sample_prediction.eval({sample_input: np.array([feed])})\n",
    "            feed = sample_distribution(prediction[0])\n",
    "            sentence += id2bi[feed]\n",
    "          print('--> ' + sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = bi_valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, one_hot(b[1], n_bigrams))\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Didn't have time. Idea is to tie up two LSTMs. First (encoder) \"memorizes\" data\n",
    "# into state and outputs the latest state. Second (decoder) tries to decode the data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
